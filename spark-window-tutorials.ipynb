{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0aff464",
   "metadata": {},
   "source": [
    "# Window Functions\n",
    "\n",
    "* Window functions operate on a set of rows and return a single aggregated value for each row. The term Window describes the set of rows in the database on which the function will operate.\n",
    "\n",
    "* We define the Window (set of rows on which functions operates) using an OVER() clause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683847c7",
   "metadata": {},
   "source": [
    "### Types of Window functions\n",
    "\n",
    "* Aggregate Window Functions SUM(), MAX(), MIN(), AVG(). COUNT()\n",
    "* Ranking Window Functions RANK(), DENSE_RANK(), ROW_NUMBER(), NTILE()\n",
    "* Value Window Functions LAG(), LEAD(), FIRST_VALUE(), LAST_VALUE()\n",
    "\n",
    "#### OVER\n",
    "* Specifies the window clauses for aggregate functions.\n",
    "\n",
    "#### PARTITION BY partition_list\n",
    "* \n",
    "\n",
    "### Useful links\n",
    "\n",
    "[Good Example in sql Server](https://www.sqlshack.com/use-window-functions-sql-server/)\n",
    "\n",
    "[Interview window functions](https://towardsdatascience.com/sql-window-function-demonstrated-with-real-interview-questions-from-leetcode-e83e28edaabc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0de8fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, array, when, array_remove, lit, expr,to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8b4552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Python Spark SQL Window Example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7780ca8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43ee38",
   "metadata": {},
   "source": [
    "### Create Spark Sample Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "edb8ad67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     city|total_order_amount|\n",
      "+---------+------------------+\n",
      "|GuildFord|             50500|\n",
      "|Arlington|             37000|\n",
      "| Shalford|             13000|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('1001','2017-04-01','David Smith','GuildFord',10000),\n",
    "('1002','2017-04-02','David Jones','Arlington',20000),\n",
    "('1003','2017-04-03','John Smith','Shalford',5000),\n",
    "('1004','2017-04-04','Michael Smith','GuildFord',15000),\n",
    "('1005','2017-04-05','David Williams','Shalford',7000),\n",
    "('1006','2017-04-06','Paum Smith','GuildFord',25000),\n",
    "('1007','2017-04-10','Andrew Smith','Arlington',15000),\n",
    "('1008','2017-04-11','David Brown','Arlington',2000),\n",
    "('1009','2017-04-20','Robert Smith','Shalford',1000),\n",
    "('1010','2017-04-25','Peter Smith','GuildFord',500)]\n",
    "\n",
    "schema = \"order_id STRING, order_date STRING, customer_name STRING, city STRING, order_amount INT\"\n",
    "\n",
    "ordersDF = spark.createDataFrame(data, schema)\n",
    "ordersDF = ordersDF.withColumn(\"order_date\", to_date(col(\"order_date\"), 'yyyy-mm-dd').cast(\"DATE\"))\n",
    "ordersDF.createOrReplaceTempView(\"Orders\")\n",
    "sumDF = spark.sql(\"select city, sum(order_amount) total_order_amount from Orders group by City\")\n",
    "sumDF.show(10)\n",
    "\n",
    "\n",
    "\n",
    "# SELECT city, SUM(order_amount) total_order_amount FROM [dbo].[Orders] GROUP BY city\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a4d717ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+---------+------------+-----------+\n",
      "|order_id|order_date| customer_name|     city|order_amount|grand_total|\n",
      "+--------+----------+--------------+---------+------------+-----------+\n",
      "|    1001|2017-01-01|   David Smith|GuildFord|       10000|      50500|\n",
      "|    1004|2017-01-04| Michael Smith|GuildFord|       15000|      50500|\n",
      "|    1006|2017-01-06|    Paum Smith|GuildFord|       25000|      50500|\n",
      "|    1010|2017-01-25|   Peter Smith|GuildFord|         500|      50500|\n",
      "|    1002|2017-01-02|   David Jones|Arlington|       20000|      37000|\n",
      "|    1007|2017-01-10|  Andrew Smith|Arlington|       15000|      37000|\n",
      "|    1008|2017-01-11|   David Brown|Arlington|        2000|      37000|\n",
      "|    1003|2017-01-03|    John Smith| Shalford|        5000|      13000|\n",
      "|    1005|2017-01-05|David Williams| Shalford|        7000|      13000|\n",
      "|    1009|2017-01-20|  Robert Smith| Shalford|        1000|      13000|\n",
      "+--------+----------+--------------+---------+------------+-----------+\n",
      "\n",
      "+--------+----------+--------------+---------+------------+-----+\n",
      "|order_id|order_date| customer_name|     city|order_amount|  avg|\n",
      "+--------+----------+--------------+---------+------------+-----+\n",
      "|    1003|2017-01-03|    John Smith| Shalford|        5000|13000|\n",
      "|    1005|2017-01-05|David Williams| Shalford|        7000|13000|\n",
      "|    1009|2017-01-20|  Robert Smith| Shalford|        1000|13000|\n",
      "|    1002|2017-01-02|   David Jones|Arlington|       20000|37000|\n",
      "|    1007|2017-01-10|  Andrew Smith|Arlington|       15000|37000|\n",
      "|    1008|2017-01-11|   David Brown|Arlington|        2000|37000|\n",
      "|    1001|2017-01-01|   David Smith|GuildFord|       10000|50500|\n",
      "|    1004|2017-01-04| Michael Smith|GuildFord|       15000|50500|\n",
      "|    1006|2017-01-06|    Paum Smith|GuildFord|       25000|50500|\n",
      "|    1010|2017-01-25|   Peter Smith|GuildFord|         500|50500|\n",
      "+--------+----------+--------------+---------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sumDF = spark.sql(\"select order_id, order_date, customer_name, city, order_amount, sum(order_amount) over (partition by city) as grand_total from Orders\")\n",
    "sumDF.show(10)\n",
    "\n",
    "avgDF = spark.sql(\"select order_id, order_date, customer_name, city, order_amount, sum(order_amount) over (partition by city, month(order_date)) as avg from Orders\")\n",
    "avgDF.show(10)\n",
    "\n",
    "# Can calculate\n",
    "# max(order_amount)\n",
    "# min(order_amount)\n",
    "# count(order_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34920a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb28c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6378b8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n",
      "+-----+-----------+------+---+\n",
      "| name|       dept|salary|age|\n",
      "+-----+-----------+------+---+\n",
      "| Lisa|      Sales| 10000| 35|\n",
      "| Evan|      Sales| 32000| 38|\n",
      "| Fred|Engineering| 21000| 28|\n",
      "| Alex|      Sales| 30000| 33|\n",
      "|  Tom|Engineering| 23000| 33|\n",
      "| Jane|  Marketing| 29000| 28|\n",
      "| Jeff|  Marketing| 35000| 38|\n",
      "| Paul|Engineering| 29000| 23|\n",
      "|Chloe|Engineering| 23000| 25|\n",
      "+-----+-----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Lisa\", \"Sales\", 10000, 35),(\"Evan\", \"Sales\", 32000, 38),\n",
    "        (\"Fred\", \"Engineering\", 21000, 28),\n",
    "        (\"Alex\", \"Sales\", 30000, 33),\n",
    "        (\"Tom\", \"Engineering\", 23000, 33),\n",
    "        (\"Jane\", \"Marketing\", 29000, 28),\n",
    "        (\"Jeff\", \"Marketing\", 35000, 38),\n",
    "        (\"Paul\", \"Engineering\", 29000, 23),\n",
    "        (\"Chloe\", \"Engineering\", 23000, 25)]\n",
    "\n",
    "df = spark.createDataFrame(data, \"name STRING, dept STRING, salary INT, age INT\")\n",
    "df.printSchema()\n",
    "df.show(10)\n",
    "df.createOrReplaceTempView(\"employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628508e5",
   "metadata": {},
   "source": [
    "### RANK()\n",
    "\n",
    "* The RANK() function is used to give a unique rank to each record based on a specified value, for example salary, order amount etc.\n",
    "\n",
    "* If two records have the same value then the RANK() function will assign the same rank to both records by skipping the next rank. This means – if there are two identical values at rank 2, it will assign the same rank 2 to both records and then skip rank 3 and assign rank 4 to the next record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efe7979b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------+----+\n",
      "| name|       dept|salary|rank|\n",
      "+-----+-----------+------+----+\n",
      "| Evan|      Sales| 32000|   1|\n",
      "| Alex|      Sales| 30000|   2|\n",
      "| Lisa|      Sales| 10000|   3|\n",
      "| Paul|Engineering| 29000|   1|\n",
      "|  Tom|Engineering| 23000|   2|\n",
      "|Chloe|Engineering| 23000|   2|\n",
      "| Fred|Engineering| 21000|   4|\n",
      "| Jeff|  Marketing| 35000|   1|\n",
      "| Jane|  Marketing| 29000|   2|\n",
      "+-----+-----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_rank = spark.sql(\"SELECT name, dept,salary, RANK() OVER (PARTITION BY dept ORDER BY salary desc) AS rank FROM employees\")\n",
    "df2_rank.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d87363",
   "metadata": {},
   "source": [
    "From the above image, you can see that the same rank (3) is assigned to two identical records (each having an order amount of 15,000) and it then skips the next rank (4) and assign rank 5 to next record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7c6ca",
   "metadata": {},
   "source": [
    "### DENSE_RANK()\n",
    "\n",
    "The DENSE_RANK() function is identical to the RANK() function except that it does not skip any rank. This means that if two identical records are found then DENSE_RANK() will assign the same rank to both records but not skip then skip the next rank.\n",
    "\n",
    "Let’s see how this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "332b8b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------+----------+\n",
      "| name|       dept|salary|dense_rank|\n",
      "+-----+-----------+------+----------+\n",
      "| Evan|      Sales| 32000|         1|\n",
      "| Alex|      Sales| 30000|         2|\n",
      "| Lisa|      Sales| 10000|         3|\n",
      "| Paul|Engineering| 29000|         1|\n",
      "|  Tom|Engineering| 23000|         2|\n",
      "|Chloe|Engineering| 23000|         2|\n",
      "| Fred|Engineering| 21000|         3|\n",
      "| Jeff|  Marketing| 35000|         1|\n",
      "| Jane|  Marketing| 29000|         2|\n",
      "+-----+-----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3_dense_rank = spark.sql(\"SELECT name, dept,salary, DENSE_RANK() OVER (PARTITION BY dept ORDER BY salary desc ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank FROM employees\")\n",
    "df3_dense_rank.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf86183",
   "metadata": {},
   "source": [
    "### ROW_NUMBER()\n",
    "#### ROW_ NUMBER() without PARTITION BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f3cef577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+---------+------------+----------+\n",
      "|order_id|order_date| customer_name|     city|order_amount|row_number|\n",
      "+--------+----------+--------------+---------+------------+----------+\n",
      "|    1001|2017-01-01|   David Smith|GuildFord|       10000|         1|\n",
      "|    1002|2017-01-02|   David Jones|Arlington|       20000|         2|\n",
      "|    1003|2017-01-03|    John Smith| Shalford|        5000|         3|\n",
      "|    1004|2017-01-04| Michael Smith|GuildFord|       15000|         4|\n",
      "|    1005|2017-01-05|David Williams| Shalford|        7000|         5|\n",
      "|    1006|2017-01-06|    Paum Smith|GuildFord|       25000|         6|\n",
      "|    1007|2017-01-10|  Andrew Smith|Arlington|       15000|         7|\n",
      "|    1008|2017-01-11|   David Brown|Arlington|        2000|         8|\n",
      "|    1009|2017-01-20|  Robert Smith| Shalford|        1000|         9|\n",
      "|    1010|2017-01-25|   Peter Smith|GuildFord|         500|        10|\n",
      "+--------+----------+--------------+---------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rownumDF = spark.sql(\"SELECT order_id,order_date,customer_name,city, order_amount, ROW_NUMBER() OVER(ORDER BY order_id) row_number FROM Orders\")\n",
    "rownumDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63281f7d",
   "metadata": {},
   "source": [
    " ## ROW_NUMBER() with PARTITION BY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af0187b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+---------+------------+----------+\n",
      "|order_id|order_date| customer_name|     city|order_amount|row_number|\n",
      "+--------+----------+--------------+---------+------------+----------+\n",
      "|    1006|2017-01-06|    Paum Smith|GuildFord|       25000|         1|\n",
      "|    1004|2017-01-04| Michael Smith|GuildFord|       15000|         2|\n",
      "|    1001|2017-01-01|   David Smith|GuildFord|       10000|         3|\n",
      "|    1010|2017-01-25|   Peter Smith|GuildFord|         500|         4|\n",
      "|    1002|2017-01-02|   David Jones|Arlington|       20000|         1|\n",
      "|    1007|2017-01-10|  Andrew Smith|Arlington|       15000|         2|\n",
      "|    1008|2017-01-11|   David Brown|Arlington|        2000|         3|\n",
      "|    1005|2017-01-05|David Williams| Shalford|        7000|         1|\n",
      "|    1003|2017-01-03|    John Smith| Shalford|        5000|         2|\n",
      "|    1009|2017-01-20|  Robert Smith| Shalford|        1000|         3|\n",
      "+--------+----------+--------------+---------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnPar = spark.sql(\"SELECT order_id,order_date,customer_name,city, order_amount, ROW_NUMBER() OVER(PARTITION BY city ORDER BY order_amount DESC) as row_number FROM Orders\")\n",
    "rnPar.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42e585",
   "metadata": {},
   "source": [
    "## NTILE()\n",
    "\n",
    "* NTILE() is a very helpful window function. It helps you to identify what percentile (or quartile, or any other subdivision) a given row falls into.\n",
    "\n",
    "* This means that if you have 100 rows and you want to create 4 quartiles based on a specified value field you can do so easily and see how many rows fall into each quartile.\n",
    "\n",
    "* Let’s see an example. In the query below, we have specified that we want to create four quartiles based on order amount. We then want to see how many orders fall into each quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "587f7896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+---------+------------+----------+\n",
      "|order_id|order_date| customer_name|     city|order_amount|row_number|\n",
      "+--------+----------+--------------+---------+------------+----------+\n",
      "|    1010|2017-01-25|   Peter Smith|GuildFord|         500|         1|\n",
      "|    1009|2017-01-20|  Robert Smith| Shalford|        1000|         1|\n",
      "|    1008|2017-01-11|   David Brown|Arlington|        2000|         1|\n",
      "|    1003|2017-01-03|    John Smith| Shalford|        5000|         2|\n",
      "|    1005|2017-01-05|David Williams| Shalford|        7000|         2|\n",
      "|    1001|2017-01-01|   David Smith|GuildFord|       10000|         2|\n",
      "|    1004|2017-01-04| Michael Smith|GuildFord|       15000|         3|\n",
      "|    1007|2017-01-10|  Andrew Smith|Arlington|       15000|         3|\n",
      "|    1002|2017-01-02|   David Jones|Arlington|       20000|         4|\n",
      "|    1006|2017-01-06|    Paum Smith|GuildFord|       25000|         4|\n",
      "+--------+----------+--------------+---------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ntilePar = spark.sql(\"SELECT order_id,order_date,customer_name,city, order_amount, NTILE(4) OVER(ORDER BY order_amount) as row_number from Orders\")\n",
    "ntilePar.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7591bea2",
   "metadata": {},
   "source": [
    "* NTILE creates tiles based on following formula:\n",
    "\n",
    "* No of rows in each tile = number of rows in result set / number of tiles specified\n",
    "\n",
    "* Here is our example, we have total 10 rows and 4 tiles are specified in the query so number of rows in each tile will be 2.5 (10/4). As number of rows should be whole number, not a decimal. SQL engine will assign 3 rows for first two groups and 2 rows for remaining two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e459d9d8",
   "metadata": {},
   "source": [
    "[Datascience](https://towardsdatascience.com/sql-window-function-demonstrated-with-real-interview-questions-from-leetcode-e83e28edaabc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
